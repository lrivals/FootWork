{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 \u2014 Entra\u00eenement des mod\u00e8les multiclasse\n",
    "\n",
    "Entra\u00eene 10 classifiers sur `HomeWin / Draw / AwayWin` avec split temporel (train < 2022, test \u2265 2022).\n",
    "\n",
    "**Pr\u00e9-requis :** avoir g\u00e9n\u00e9r\u00e9 `data/all_leagues_combined.csv` avec le notebook `01_Data_Processing.ipynb`.\n",
    "\n",
    "---\n",
    "### Google Colab : configuration du chemin Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION \u2014 \u00c0 modifier si n\u00e9cessaire\n",
    "# ============================================================\n",
    "DRIVE_PROJECT_PATH = '/content/drive/MyDrive/FootWork'\n",
    "LOCAL_PROJECT_PATH = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# D\u00e9tection environnement + montage Drive\n",
    "# ============================================================\n",
    "import os, sys\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    ON_COLAB = True\n",
    "except ImportError:\n",
    "    ON_COLAB = False\n",
    "\n",
    "if ON_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_ROOT = DRIVE_PROJECT_PATH\n",
    "else:\n",
    "    PROJECT_ROOT = os.path.abspath(LOCAL_PROJECT_PATH)\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "print(f'Environnement : {\"Google Colab\" if ON_COLAB else \"Local\"}')\n",
    "print(f'R\u00e9pertoire de travail : {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Installation des d\u00e9pendances\n# ============================================================\nif ON_COLAB:\n    %pip install -q pyyaml xgboost lightgbm catboost scikit-learn seaborn tqdm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n# Imports\n# ============================================================\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\nfrom sklearn.metrics import (confusion_matrix, classification_report, accuracy_score,\n                              balanced_accuracy_score, matthews_corrcoef, cohen_kappa_score,\n                              roc_curve, auc)\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\nfrom sklearn.frozen import FrozenEstimator\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n                               AdaBoostClassifier, ExtraTreesClassifier)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom src.Config.Config_Manager import ConfigManager\nfrom src.Models.threshold_optimizer import (\n    find_optimal_thresholds_multiclass, predict_with_thresholds\n)\n\nconfig = ConfigManager('src/Config/configMC_1.yaml')\nprint('Config charg\u00e9e.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Chargement et split temporel des donn\u00e9es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = config.get_paths()['full_dataset']\n",
    "exclude_columns = config.get_config_value('excluded_columns', default=[])\n",
    "split_config = config.get_config_value('data_split', default={})\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "print(f'Dataset charg\u00e9 : {len(df)} matchs \u00d7 {len(df.columns)} colonnes')\n",
    "print(f'\\nDistribution cible :')\n",
    "print(df['target_result'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split temporel :\n#   train  : year < 2020\n#   cal    : 2020 <= year < 2022  (calibration + optimisation des seuils)\n#   test   : year >= 2022\ntemporal_split_year = split_config.get('temporal_split_year', 2022)\ncal_split_year      = split_config.get('cal_split_year', 2020)\n\nif 'date' in df.columns:\n    df['date'] = pd.to_datetime(df['date'])\n    train_df = df[df['date'].dt.year < cal_split_year].copy()\n    cal_df   = df[(df['date'].dt.year >= cal_split_year) &\n                  (df['date'].dt.year < temporal_split_year)].copy()\n    test_df  = df[df['date'].dt.year >= temporal_split_year].copy()\n    print(f'Split temporel :')\n    print(f'  train  < {cal_split_year}        \u2192 {len(train_df)} matchs')\n    print(f'  cal    {cal_split_year}\u2013{temporal_split_year-1}      \u2192 {len(cal_df)} matchs')\n    print(f'  test  >= {temporal_split_year}       \u2192 {len(test_df)} matchs')\nelse:\n    random_params = {k: v for k, v in split_config.items() if k != 'temporal_split_year'}\n    train_df, test_df = train_test_split(df, **random_params)\n    cal_df = None\n    print(f'Split al\u00e9atoire : train {len(train_df)} | test {len(test_df)}')\n\ncols_to_drop = [c for c in exclude_columns if c in df.columns and c != 'target_result']\nX_train = train_df.drop(cols_to_drop + ['target_result'], axis=1, errors='ignore')\nX_test  = test_df.drop(cols_to_drop + ['target_result'], axis=1, errors='ignore')\nX_cal   = cal_df.drop(cols_to_drop + ['target_result'], axis=1, errors='ignore') if cal_df is not None else None\n\nle = LabelEncoder()\nle.fit(df['target_result'])\ny_train = le.transform(train_df['target_result'])\ny_test  = le.transform(test_df['target_result'])\ny_cal   = le.transform(cal_df['target_result']) if cal_df is not None else None\nclass_names = le.classes_\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled  = scaler.transform(X_test)\nX_cal_scaled   = scaler.transform(X_cal) if X_cal is not None else None\n\nprint(f'\\nFeatures utilis\u00e9es : {X_train.shape[1]}')\nprint(f'Classes : {class_names}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## D\u00e9finition des mod\u00e8les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = config.get_config_value('model_parameters', default={})\n",
    "\n",
    "catboost_dir = Path('src/Models/Multiclass_Target/catboost_info')\n",
    "catboost_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "models = {\n",
    "    'Random Forest':      RandomForestClassifier(**model_params.get('random_forest', {})),\n",
    "    'Logistic Regression': LogisticRegression(**model_params.get('logistic_regression', {})),\n",
    "    'SVM':                SVC(**model_params.get('svm', {})),\n",
    "    'Gradient Boosting':  GradientBoostingClassifier(**model_params.get('gradient_boosting', {})),\n",
    "    'XGBoost':            XGBClassifier(**model_params.get('xgboost', {})),\n",
    "    'LightGBM':           LGBMClassifier(**model_params.get('lightgbm', {})),\n",
    "    'CatBoost':           CatBoostClassifier(**model_params.get('catboost', {}),\n",
    "                              train_dir=str(catboost_dir)),\n",
    "    'KNN':                KNeighborsClassifier(**model_params.get('knn', {})),\n",
    "    'AdaBoost':           AdaBoostClassifier(**model_params.get('adaboost', {})),\n",
    "    'Extra Trees':        ExtraTreesClassifier(**model_params.get('extra_trees', {})),\n",
    "}\n",
    "print(f'{len(models)} mod\u00e8les pr\u00eats.')"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# Helper : entra\u00eenement avec suivi des courbes de loss\n# ============================================================\ndef _fit_with_tracking(model, X_train, X_test, y_train, y_test, sample_weight=None):\n    \"\"\"Entra\u00eene et capture les m\u00e9triques par it\u00e9ration pour les boosters.\n\n    sample_weight est propag\u00e9 aux mod\u00e8les qui n'acceptent pas class_weight\n    dans leur constructeur (XGBoost, GradientBoosting, AdaBoost).\n    \"\"\"\n    cls = model.__class__.__name__\n\n    if cls == 'XGBClassifier':\n        model.fit(X_train, y_train,\n                  sample_weight=sample_weight,\n                  eval_set=[(X_train, y_train), (X_test, y_test)],\n                  verbose=False)\n        evals = model.evals_result()\n        mk = list(evals['validation_0'].keys())[0]\n        return model, {'train': evals['validation_0'][mk],\n                       'val':   evals['validation_1'][mk], 'metric': mk}\n\n    elif cls == 'LGBMClassifier':\n        evals_result = {}\n        model.fit(X_train, y_train,\n                  eval_set=[(X_train, y_train), (X_test, y_test)],\n                  eval_names=['train', 'val'],\n                  callbacks=[lgb.record_evaluation(evals_result)])\n        mk = list(evals_result['train'].keys())[0]\n        return model, {'train': evals_result['train'][mk],\n                       'val':   evals_result['val'][mk], 'metric': mk}\n\n    elif cls == 'CatBoostClassifier':\n        base_dir = model.get_param('train_dir') or 'catboost_info'\n        safe_dir = os.path.join(base_dir, f'run_{id(model)}')\n        os.makedirs(safe_dir, exist_ok=True)\n        model.set_params(train_dir=safe_dir)\n        model.fit(X_train, y_train, eval_set=(X_test, y_test))\n        evals = model.get_evals_result()\n        lk = 'learn'      if 'learn'      in evals else list(evals.keys())[0]\n        vk = 'validation' if 'validation' in evals else list(evals.keys())[-1]\n        mk = list(evals[lk].keys())[0]\n        return model, {'train': evals[lk][mk], 'val': evals[vk][mk], 'metric': mk}\n\n    elif cls == 'GradientBoostingClassifier':\n        model.fit(X_train, y_train, sample_weight=sample_weight)\n        t = [accuracy_score(y_train, p) for p in model.staged_predict(X_train)]\n        v = [accuracy_score(y_test,  p) for p in model.staged_predict(X_test)]\n        return model, {'train': t, 'val': v, 'metric': 'accuracy'}\n\n    elif cls == 'AdaBoostClassifier':\n        model.fit(X_train, y_train, sample_weight=sample_weight)\n        return model, None\n\n    else:\n        model.fit(X_train, y_train)\n        return model, None\n\nprint('_fit_with_tracking d\u00e9fini.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "output_dir = Path(config.get_paths()['output_dir'])\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Sample weights for models without native class_weight support\nsample_weight = compute_sample_weight('balanced', y_train)\n\nresults = {}\n\npbar = tqdm(models.items(), total=len(models), desc=\"Training models\")\nfor name, model in pbar:\n    pbar.set_description(f\"Training  {name:<22}\")\n    try:\n        model, loss_history = _fit_with_tracking(\n            model, X_train_scaled, X_test_scaled, y_train, y_test,\n            sample_weight=sample_weight\n        )\n        y_pred = model.predict(X_test_scaled)\n\n        # \u2500\u2500 M\u00e9triques \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        acc     = accuracy_score(y_test, y_pred)\n        bal_acc = balanced_accuracy_score(y_test, y_pred)\n        mcc     = matthews_corrcoef(y_test, y_pred)\n        kappa   = cohen_kappa_score(y_test, y_pred)\n        cm      = confusion_matrix(y_test, y_pred)\n        cr      = classification_report(y_test, y_pred, target_names=class_names)\n        pbar.set_postfix(acc=f\"{acc:.4f}\", mcc=f\"{mcc:.4f}\")\n\n        # \u2500\u2500 ROC multiclasse (OvR) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        macro_auc, per_class_auc, probs = None, {}, None\n        if hasattr(model, 'predict_proba'):\n            try:\n                probs = model.predict_proba(X_test_scaled)\n                n = len(class_names)\n                y_bin = label_binarize(y_test, classes=list(range(n)))\n                fpr_d, tpr_d, auc_d = {}, {}, {}\n                for i in range(n):\n                    fpr_d[i], tpr_d[i], _ = roc_curve(y_bin[:, i], probs[:, i])\n                    auc_d[i] = auc(fpr_d[i], tpr_d[i])\n                per_class_auc = auc_d\n                all_fpr  = np.unique(np.concatenate([fpr_d[i] for i in range(n)]))\n                mean_tpr = np.zeros_like(all_fpr)\n                for i in range(n):\n                    mean_tpr += np.interp(all_fpr, fpr_d[i], tpr_d[i])\n                mean_tpr /= n\n                macro_auc = auc(all_fpr, mean_tpr)\n\n                fig, ax = plt.subplots(figsize=(6, 5))\n                colors = plt.cm.Set1(np.linspace(0, 0.8, n))\n                for i, (cls_name, color) in enumerate(zip(class_names, colors)):\n                    ax.plot(fpr_d[i], tpr_d[i], color=color, lw=2,\n                            label=f'{cls_name} (AUC={auc_d[i]:.3f})')\n                ax.plot(all_fpr, mean_tpr, 'k--', lw=2,\n                        label=f'Macro avg (AUC={macro_auc:.3f})')\n                ax.plot([0,1],[0,1],'gray',ls=':',lw=1)\n                ax.set_title(f'ROC OvR \u2014 {name}')\n                ax.set_xlabel('FPR'); ax.set_ylabel('TPR')\n                ax.legend(loc='lower right', fontsize=8)\n                fig.tight_layout()\n                fig.savefig(output_dir / f'roc_{name.replace(\" \",\"_\")}.png', dpi=150)\n                plt.show()\n            except Exception as e:\n                print(f'  [ROC skipped]: {e}')\n\n        # \u2500\u2500 Matrice de confusion \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        fig, ax = plt.subplots(figsize=(6, 5))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                    xticklabels=class_names, yticklabels=class_names, ax=ax)\n        ax.set_title(f'{name}  Acc={acc:.3f}  MCC={mcc:.3f}')\n        ax.set_ylabel('Vrai label'); ax.set_xlabel('Label pr\u00e9dit')\n        fig.tight_layout()\n        fig.savefig(output_dir / f'cm_{name.replace(\" \",\"_\")}.png', dpi=150)\n        plt.show()\n\n        # \u2500\u2500 Courbe de loss (boosters uniquement) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        if loss_history:\n            lh = loss_history\n            _LOSS_METRICS = {'loss', 'logloss', 'multiclass', 'merror', 'mae', 'mse', 'rmse'}\n            is_loss = any(tok in lh['metric'].lower() for tok in _LOSS_METRICS)\n            best = int(np.argmin(lh['val']) if is_loss else np.argmax(lh['val']))\n            fig, ax = plt.subplots(figsize=(7, 4))\n            iters = range(1, len(lh['train']) + 1)\n            ax.plot(iters, lh['train'], label='Train')\n            ax.plot(iters, lh['val'],   label='Validation')\n            ax.axvline(best + 1, color='red', ls='--', alpha=0.6, label=f'Best={best+1}')\n            ax.set_title(f'Training Curve \u2014 {name}  ({lh[\"metric\"]})')\n            ax.set_xlabel('It\u00e9ration'); ax.set_ylabel(lh['metric'])\n            ax.legend(); fig.tight_layout()\n            fig.savefig(output_dir / f'loss_curve_{name.replace(\" \",\"_\")}.png', dpi=150)\n            plt.show()\n\n        # \u2500\u2500 Calibration + seuils optimis\u00e9s \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        # Run sequentially here (no threading) so exceptions are visible.\n        calibrated_model, calibrated_auc = None, None\n        opt_thresholds, acc_opt, bal_acc_opt, report_opt = None, None, None, None\n\n        if X_cal_scaled is not None and probs is not None:\n            try:\n                calibrated_model = CalibratedClassifierCV(FrozenEstimator(model), method='isotonic')\n                calibrated_model.fit(X_cal_scaled, y_cal)\n\n                cal_probs_test = calibrated_model.predict_proba(X_test_scaled)\n                n = len(class_names)\n                y_bin = label_binarize(y_test, classes=list(range(n)))\n                all_fpr  = np.unique(np.concatenate([\n                    roc_curve(y_bin[:, i], cal_probs_test[:, i])[0] for i in range(n)\n                ]))\n                mean_tpr = np.zeros_like(all_fpr)\n                for i in range(n):\n                    fpr_i, tpr_i, _ = roc_curve(y_bin[:, i], cal_probs_test[:, i])\n                    mean_tpr += np.interp(all_fpr, fpr_i, tpr_i)\n                mean_tpr /= n\n                calibrated_auc = auc(all_fpr, mean_tpr)\n\n                opt_thresholds = find_optimal_thresholds_multiclass(\n                    calibrated_model, X_cal_scaled, y_cal\n                )\n                y_pred_opt = predict_with_thresholds(cal_probs_test, opt_thresholds)\n                acc_opt     = accuracy_score(y_test, y_pred_opt)\n                bal_acc_opt = balanced_accuracy_score(y_test, y_pred_opt)\n                report_opt  = classification_report(y_test, y_pred_opt, target_names=class_names)\n\n                print(f'  {name}: cal_auc={calibrated_auc:.4f}  acc_opt={acc_opt:.4f}')\n            except Exception as e:\n                import traceback\n                print(f'  [Calibration FAILED for {name}]:')\n                traceback.print_exc()\n\n        results[name] = {\n            'accuracy': acc, 'balanced_accuracy': bal_acc,\n            'mcc': mcc, 'kappa': kappa,\n            'macro_roc_auc': macro_auc, 'per_class_auc': per_class_auc,\n            'loss_history': loss_history, 'confusion_matrix': cm, 'report': cr,\n            'calibrated_model': calibrated_model,\n            'calibrated_macro_auc': calibrated_auc,\n            'optimal_thresholds': opt_thresholds,\n            'accuracy_opt': acc_opt,\n            'balanced_accuracy_opt': bal_acc_opt,\n            'report_opt': report_opt,\n        }\n\n    except Exception as e:\n        print(f'  ERREUR : {e}')\n        import traceback; traceback.print_exc()\n\nprint('\\n=== Entra\u00eenement termin\u00e9 ===')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Tableau de classement complet\nrows = []\nfor name, res in results.items():\n    row = {\n        'Mod\u00e8le':    name,\n        'Accuracy':  res['accuracy'],\n        'Bal. Acc':  res['balanced_accuracy'],\n        'MCC':       res['mcc'],\n        'Kappa':     res['kappa'],\n        'Macro AUC': res.get('macro_roc_auc') or 0.0,\n    }\n    for i, cls_name in enumerate(class_names):\n        row[f'AUC {cls_name}'] = res['per_class_auc'].get(i, np.nan)\n    rows.append(row)\n\nsummary = pd.DataFrame(rows).sort_values('Accuracy', ascending=False)\npd.options.display.float_format = '{:.4f}'.format\nprint(summary.to_string(index=False))\n\n# Graphique comparatif\nmodel_names = summary['Mod\u00e8le'].tolist()\nx, w = np.arange(len(model_names)), 0.35\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nax = axes[0]\nax.bar(x - w/2, summary['Accuracy'],  w, label='Accuracy',     color='steelblue')\nax.bar(x + w/2, summary['Bal. Acc'],  w, label='Balanced Acc', color='coral')\nax.set_xticks(x); ax.set_xticklabels(model_names, rotation=30, ha='right')\nax.set_ylim([0, 1]); ax.set_ylabel('Score'); ax.legend()\nax.set_title('Accuracy vs Balanced Accuracy')\n\nax = axes[1]\nax.bar(x, summary['Macro AUC'], color='mediumseagreen')\nax.axhline(0.5, color='red', ls='--', alpha=0.5, label='Random')\nax.set_xticks(x); ax.set_xticklabels(model_names, rotation=30, ha='right')\nax.set_ylim([0, 1]); ax.set_ylabel('Macro AUC (OvR)'); ax.legend()\nax.set_title('Macro AUC par mod\u00e8le')\n\nfig.tight_layout()\nfig.savefig(output_dir / 'summary_comparison.png', dpi=150)\nplt.show()"
  },
  {
   "cell_type": "code",
   "source": "metrics_file = output_dir / 'metrics_results_Multiclass.txt'\nsorted_res = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n\nwith open(metrics_file, 'w') as f:\n    f.write('Results \u2014 Multiclass Prediction (temporal split)\\n')\n    f.write('=' * 70 + '\\n\\n')\n\n    # Ranking table\n    f.write('=== RANKING TABLE ===\\n')\n    f.write(f\"{'Mod\u00e8le':<22} {'Accuracy':>9} {'Bal.Acc':>9} {'MCC':>8} {'Kappa':>8} {'MacroAUC':>10}\\n\")\n    f.write('-' * 72 + '\\n')\n    for name, res in sorted_res:\n        auc_str = f\"{res['macro_roc_auc']:.4f}\" if res.get('macro_roc_auc') else '  N/A  '\n        f.write(f\"{name:<22} {res['accuracy']:>9.4f} {res['balanced_accuracy']:>9.4f} \"\n                f\"{res['mcc']:>8.4f} {res['kappa']:>8.4f} {auc_str:>10}\\n\")\n    f.write('\\n')\n\n    # Per-class AUC\n    f.write('=== PER-CLASS AUC (OvR) ===\\n')\n    f.write(f\"{'Mod\u00e8le':<22}\" + ''.join(f'{c:>12}' for c in class_names) + '\\n')\n    f.write('-' * (22 + 12 * len(class_names)) + '\\n')\n    for name, res in sorted_res:\n        row = f'{name:<22}'\n        for i in range(len(class_names)):\n            v = res['per_class_auc'].get(i)\n            row += f'{v:>12.4f}' if v is not None else f\"{'N/A':>12}\"\n        f.write(row + '\\n')\n    f.write('\\n')\n\n    # Calibrated vs Uncalibrated\n    f.write('=== CALIBRATED vs UNCALIBRATED MACRO AUC ===\\n')\n    f.write(f\"{'Mod\u00e8le':<22} {'Uncalibrated':>14} {'Calibrated':>12} {'Delta':>8}\\n\")\n    f.write('-' * 60 + '\\n')\n    for name, res in sorted_res:\n        raw = res.get('macro_roc_auc')\n        cal = res.get('calibrated_macro_auc')\n        raw_s = f\"{raw:.4f}\" if raw is not None else \"   N/A\"\n        cal_s = f\"{cal:.4f}\" if cal is not None else \"   N/A\"\n        delta_s = f\"{cal - raw:+.4f}\" if (raw is not None and cal is not None) else \"   N/A\"\n        f.write(f\"{name:<22} {raw_s:>14} {cal_s:>12} {delta_s:>8}\\n\")\n    f.write('\\n')\n\n    # Optimised threshold metrics\n    f.write('=== OPTIMISED THRESHOLD METRICS ===\\n')\n    f.write(f\"{'Mod\u00e8le':<22} {'Acc(default)':>13} {'Acc(opt)':>10} {'BalAcc(default)':>16} {'BalAcc(opt)':>12}\\n\")\n    f.write('-' * 77 + '\\n')\n    for name, res in sorted_res:\n        acc_o_s = f\"{res['accuracy_opt']:.4f}\" if res.get('accuracy_opt') is not None else \"   N/A\"\n        bal_o_s = f\"{res['balanced_accuracy_opt']:.4f}\" if res.get('balanced_accuracy_opt') is not None else \"   N/A\"\n        f.write(f\"{name:<22} {res['accuracy']:>13.4f} {acc_o_s:>10} \"\n                f\"{res['balanced_accuracy']:>16.4f} {bal_o_s:>12}\\n\")\n    f.write('\\n')\n\n    # Detailed\n    f.write('=== DETAILED RESULTS ===\\n\\n')\n    for name, res in sorted_res:\n        f.write(f'Mod\u00e8le : {name}\\n')\n        f.write('-' * 30 + '\\n')\n        f.write(f\"Accuracy          : {res['accuracy']:.4f}\\n\")\n        f.write(f\"Balanced Accuracy : {res['balanced_accuracy']:.4f}\\n\")\n        f.write(f\"MCC               : {res['mcc']:.4f}\\n\")\n        f.write(f\"Cohen's Kappa     : {res['kappa']:.4f}\\n\")\n        if res.get('macro_roc_auc'):\n            f.write(f\"Macro ROC-AUC     : {res['macro_roc_auc']:.4f}\\n\")\n        if res.get('calibrated_macro_auc'):\n            f.write(f\"Calibrated AUC    : {res['calibrated_macro_auc']:.4f}\\n\")\n        if res.get('accuracy_opt') is not None:\n            f.write(f\"Accuracy (opt)    : {res['accuracy_opt']:.4f}\\n\")\n            f.write(f\"Bal.Acc  (opt)    : {res['balanced_accuracy_opt']:.4f}\\n\")\n        f.write('\\nClassification Report :\\n')\n        f.write(res['report'])\n        if res.get('report_opt'):\n            f.write('\\nClassification Report (Optimised Thresholds) :\\n')\n            f.write(res['report_opt'])\n        if res.get('optimal_thresholds'):\n            f.write('\\nOptimal Thresholds :\\n')\n            for i, (t, f1) in res['optimal_thresholds'].items():\n                f.write(f\"  {class_names[i]}: threshold={t:.4f}, best_F1={f1:.4f}\\n\")\n        f.write('\\nConfusion Matrix :\\n')\n        f.write(str(res['confusion_matrix']))\n        lh = res.get('loss_history')\n        if lh:\n            _LOSS_METRICS = {'loss', 'logloss', 'multiclass', 'merror', 'mae', 'mse', 'rmse'}\n            is_loss = any(tok in lh['metric'].lower() for tok in _LOSS_METRICS)\n            best = int(np.argmin(lh['val']) if is_loss else np.argmax(lh['val']))\n            f.write(f\"\\nTraining curve: metric={lh['metric']}, best_iter={best+1}, \"\n                    f\"final_train={lh['train'][-1]:.4f}, final_val={lh['val'][-1]:.4f}\\n\")\n        f.write('\\n' + '=' * 70 + '\\n')\n\nprint(f'M\u00e9triques sauvegard\u00e9es : {metrics_file}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}