{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 \u2014 Entra\u00eenement des mod\u00e8les binaires\n",
    "\n",
    "Entra\u00eene 10 classifiers en formulation binaire :\n",
    "- **HomeWin vs Not-HomeWin**\n",
    "- **AwayWin vs Not-AwayWin**\n",
    "\n",
    "Avec courbes ROC et split temporel (train < 2022, test \u2265 2022).\n",
    "\n",
    "**Pr\u00e9-requis :** avoir g\u00e9n\u00e9r\u00e9 `data/all_leagues_combined.csv` avec le notebook `01_Data_Processing.ipynb`.\n",
    "\n",
    "---\n",
    "### Google Colab : configuration du chemin Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION \u2014 \u00c0 modifier si n\u00e9cessaire\n",
    "# ============================================================\n",
    "DRIVE_PROJECT_PATH = '/content/drive/MyDrive/FootWork'\n",
    "LOCAL_PROJECT_PATH = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# D\u00e9tection environnement + montage Drive\n",
    "# ============================================================\n",
    "import os, sys\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    ON_COLAB = True\n",
    "except ImportError:\n",
    "    ON_COLAB = False\n",
    "\n",
    "if ON_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_ROOT = DRIVE_PROJECT_PATH\n",
    "else:\n",
    "    PROJECT_ROOT = os.path.abspath(LOCAL_PROJECT_PATH)\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "print(f'Environnement : {\"Google Colab\" if ON_COLAB else \"Local\"}')\n",
    "print(f'R\u00e9pertoire de travail : {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if ON_COLAB:\n    %pip install -q pyyaml xgboost lightgbm catboost scikit-learn seaborn tqdm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n# Imports\n# ============================================================\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (confusion_matrix, classification_report, accuracy_score,\n                              balanced_accuracy_score, matthews_corrcoef, cohen_kappa_score,\n                              roc_curve, auc)\nfrom sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\nfrom sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\nfrom sklearn.frozen import FrozenEstimator\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n                               AdaBoostClassifier, ExtraTreesClassifier)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom src.Config.Config_Manager import ConfigManager\nfrom src.Models.threshold_optimizer import find_optimal_threshold, predict_binary_with_threshold\n\nconfig = ConfigManager('src/Config/configBT_1.yaml')\nprint('Config charg\u00e9e.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Chargement et split temporel des donn\u00e9es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = config.get_paths()['full_dataset']\n",
    "exclude_columns = config.get_config_value('excluded_columns', default=[])\n",
    "split_config = config.get_config_value('data_split', default={})\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Targets binaires\n",
    "df['home_win'] = (df['target_result'] == 'HomeWin').astype(int)\n",
    "df['away_win'] = (df['target_result'] == 'AwayWin').astype(int)\n",
    "\n",
    "print(f'Dataset : {len(df)} matchs \u00d7 {len(df.columns)} colonnes')\n",
    "print(f'HomeWin : {df[\"home_win\"].sum()} ({df[\"home_win\"].mean():.1%})')\n",
    "print(f'AwayWin : {df[\"away_win\"].sum()} ({df[\"away_win\"].mean():.1%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split temporel :\n#   train  : year < 2020\n#   cal    : 2020 <= year < 2022  (calibration + optimisation de seuil)\n#   test   : year >= 2022\ntemporal_split_year = split_config.get('temporal_split_year', 2022)\ncal_split_year      = split_config.get('cal_split_year', 2020)\ncols_to_drop = [c for c in exclude_columns + ['target_result', 'home_win', 'away_win']\n                if c in df.columns]\n\nif 'date' in df.columns:\n    df['date'] = pd.to_datetime(df['date'])\n    train_df = df[df['date'].dt.year < cal_split_year].copy()\n    cal_df   = df[(df['date'].dt.year >= cal_split_year) &\n                  (df['date'].dt.year < temporal_split_year)].copy()\n    test_df  = df[df['date'].dt.year >= temporal_split_year].copy()\n    print(f'Split temporel :')\n    print(f'  train  < {cal_split_year}        \u2192 {len(train_df)} matchs')\n    print(f'  cal    {cal_split_year}\u2013{temporal_split_year-1}      \u2192 {len(cal_df)} matchs')\n    print(f'  test  >= {temporal_split_year}       \u2192 {len(test_df)} matchs')\n    X_train = train_df.drop(cols_to_drop, axis=1, errors='ignore')\n    X_cal   = cal_df.drop(cols_to_drop, axis=1, errors='ignore')\n    X_test  = test_df.drop(cols_to_drop, axis=1, errors='ignore')\n    y_home_train, y_home_cal, y_home_test = (\n        train_df['home_win'], cal_df['home_win'], test_df['home_win']\n    )\n    y_away_train, y_away_cal, y_away_test = (\n        train_df['away_win'], cal_df['away_win'], test_df['away_win']\n    )\nelse:\n    random_params = {k: v for k, v in split_config.items() if k != 'temporal_split_year'}\n    X = df.drop(cols_to_drop, axis=1, errors='ignore')\n    X_train, X_test, y_home_train, y_home_test, y_away_train, y_away_test = train_test_split(\n        X, df['home_win'], df['away_win'], **random_params\n    )\n    X_cal, y_home_cal, y_away_cal = None, None, None\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_cal_scaled   = scaler.transform(X_cal) if X_cal is not None else None\nX_test_scaled  = scaler.transform(X_test)\n\nprint(f'\\nFeatures utilis\u00e9es : {X_train.shape[1]}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## D\u00e9finition des mod\u00e8les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = config.get_config_value('model_parameters', default={})\n",
    "\n",
    "catboost_dir = Path('src/Models/Binary_Target/catboost_info')\n",
    "catboost_dir.mkdir(parents=True, exist_ok=True)\n",
    "catboost_p = {**model_params.get('catboost', {}), 'train_dir': str(catboost_dir)}\n",
    "\n",
    "def make_models():\n",
    "    return {\n",
    "        'Random Forest':       RandomForestClassifier(**model_params.get('random_forest', {})),\n",
    "        'Logistic Regression': LogisticRegression(**model_params.get('logistic_regression', {})),\n",
    "        'SVM':                 SVC(**model_params.get('svm', {})),\n",
    "        'Gradient Boosting':   GradientBoostingClassifier(**model_params.get('gradient_boosting', {})),\n",
    "        'XGBoost':             XGBClassifier(**model_params.get('xgboost', {})),\n",
    "        'LightGBM':            LGBMClassifier(**model_params.get('lightgbm', {})),\n",
    "        'CatBoost':            CatBoostClassifier(**catboost_p),\n",
    "        'KNN':                 KNeighborsClassifier(**model_params.get('knn', {})),\n",
    "        'AdaBoost':            AdaBoostClassifier(**model_params.get('adaboost', {})),\n",
    "        'Extra Trees':         ExtraTreesClassifier(**model_params.get('extra_trees', {})),\n",
    "    }\n",
    "\n",
    "print('Mod\u00e8les d\u00e9finis.')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n# Helper : entra\u00eenement avec suivi des courbes de loss\n# ============================================================\ndef _fit_with_tracking(model, X_train, X_test, y_train, y_test, sample_weight=None):\n    \"\"\"Entra\u00eene et capture les m\u00e9triques par it\u00e9ration pour les boosters.\n\n    sample_weight est propag\u00e9 aux mod\u00e8les qui n'acceptent pas class_weight\n    dans leur constructeur (XGBoost, GradientBoosting, AdaBoost).\n    \"\"\"\n    cls = model.__class__.__name__\n\n    if cls == 'XGBClassifier':\n        model.fit(X_train, y_train,\n                  sample_weight=sample_weight,\n                  eval_set=[(X_train, y_train), (X_test, y_test)],\n                  verbose=False)\n        evals = model.evals_result()\n        mk = list(evals['validation_0'].keys())[0]\n        return model, {'train': evals['validation_0'][mk],\n                       'val':   evals['validation_1'][mk], 'metric': mk}\n\n    elif cls == 'LGBMClassifier':\n        evals_result = {}\n        model.fit(X_train, y_train,\n                  eval_set=[(X_train, y_train), (X_test, y_test)],\n                  eval_names=['train', 'val'],\n                  callbacks=[lgb.record_evaluation(evals_result)])\n        mk = list(evals_result['train'].keys())[0]\n        return model, {'train': evals_result['train'][mk],\n                       'val':   evals_result['val'][mk], 'metric': mk}\n\n    elif cls == 'CatBoostClassifier':\n        base_dir = model.get_param('train_dir') or 'catboost_info'\n        safe_dir = os.path.join(base_dir, f'run_{id(model)}')\n        os.makedirs(safe_dir, exist_ok=True)\n        model.set_params(train_dir=safe_dir)\n        model.fit(X_train, y_train, eval_set=(X_test, y_test))\n        evals = model.get_evals_result()\n        lk = 'learn'      if 'learn'      in evals else list(evals.keys())[0]\n        vk = 'validation' if 'validation' in evals else list(evals.keys())[-1]\n        mk = list(evals[lk].keys())[0]\n        return model, {'train': evals[lk][mk], 'val': evals[vk][mk], 'metric': mk}\n\n    elif cls == 'GradientBoostingClassifier':\n        model.fit(X_train, y_train, sample_weight=sample_weight)\n        t = [accuracy_score(y_train, p) for p in model.staged_predict(X_train)]\n        v = [accuracy_score(y_test,  p) for p in model.staged_predict(X_test)]\n        return model, {'train': t, 'val': v, 'metric': 'accuracy'}\n\n    elif cls == 'AdaBoostClassifier':\n        model.fit(X_train, y_train, sample_weight=sample_weight)\n        return model, None\n\n    else:\n        model.fit(X_train, y_train)\n        return model, None\n\n\ndef _train_binary(models_dict, X_train, X_test, X_cal, y_train, y_test, y_cal,\n                  target_name, label_names, output_dir):\n    \"\"\"Boucle d'entra\u00eenement commune pour HomeWin et AwayWin.\"\"\"\n    sw = compute_sample_weight('balanced', y_train)\n    results = {}\n\n    pbar = tqdm(models_dict.items(), total=len(models_dict), desc=f\"{target_name} models\")\n    for name, model in pbar:\n        pbar.set_description(f\"{target_name}  {name:<22}\")\n        try:\n            # CatBoost : class_weights dynamiques selon le d\u00e9s\u00e9quilibre r\u00e9el\n            if model.__class__.__name__ == 'CatBoostClassifier':\n                cw = compute_class_weight('balanced', classes=np.array([0, 1]),\n                                          y=np.array(y_train))\n                model.set_params(class_weights=list(cw))\n\n            model, loss_history = _fit_with_tracking(\n                model, X_train, X_test, y_train, y_test, sample_weight=sw\n            )\n            y_pred = model.predict(X_test)\n\n            acc     = accuracy_score(y_test, y_pred)\n            bal_acc = balanced_accuracy_score(y_test, y_pred)\n            mcc     = matthews_corrcoef(y_test, y_pred)\n            kappa   = cohen_kappa_score(y_test, y_pred)\n            cm      = confusion_matrix(y_test, y_pred)\n            pbar.set_postfix(acc=f\"{acc:.4f}\", mcc=f\"{mcc:.4f}\")\n\n            # Calibration + seuil optimis\u00e9 (s\u00e9quentiel \u2014 exceptions visibles)\n            calibrated_model, calibrated_roc = None, None\n            opt_thr, acc_opt, bal_opt, report_opt = None, None, None, None\n\n            if X_cal is not None and hasattr(model, 'predict_proba'):\n                try:\n                    calibrated_model = CalibratedClassifierCV(FrozenEstimator(model), method='isotonic')\n                    calibrated_model.fit(X_cal, y_cal)\n                    cal_probs_test = calibrated_model.predict_proba(X_test)[:, 1]\n                    fpr, tpr, _ = roc_curve(y_test, cal_probs_test)\n                    calibrated_roc = auc(fpr, tpr)\n\n                    # Courbe de calibration\n                    fig, ax = plt.subplots(figsize=(5, 4))\n                    raw_prob = model.predict_proba(X_cal)[:, 1]\n                    cal_prob = calibrated_model.predict_proba(X_cal)[:, 1]\n                    CalibrationDisplay.from_predictions(y_cal, raw_prob, n_bins=10, ax=ax,\n                                                        label='Uncalibrated', color='steelblue')\n                    CalibrationDisplay.from_predictions(y_cal, cal_prob, n_bins=10, ax=ax,\n                                                        label='Calibrated', color='darkorange')\n                    ax.set_title(f'Calibration \u2014 {name} ({target_name})')\n                    ax.legend(fontsize=8)\n                    fig.tight_layout()\n                    fig.savefig(output_dir / f'calibration_{target_name.replace(\" \",\"_\")}_{name.replace(\" \",\"_\")}.png', dpi=150)\n                    plt.show()\n\n                    # Seuil optimal\n                    cal_probs_cal = calibrated_model.predict_proba(X_cal)[:, 1]\n                    opt_thr_val, opt_f1 = find_optimal_threshold(cal_probs_cal, np.array(y_cal))\n                    y_pred_opt = predict_binary_with_threshold(cal_probs_test, opt_thr_val)\n                    opt_thr    = (opt_thr_val, opt_f1)\n                    acc_opt    = accuracy_score(y_test, y_pred_opt)\n                    bal_opt    = balanced_accuracy_score(y_test, y_pred_opt)\n                    report_opt = classification_report(y_test, y_pred_opt, target_names=label_names)\n\n                    print(f'  {name}: cal_auc={calibrated_roc:.4f}  acc_opt={acc_opt:.4f}')\n                except Exception as e:\n                    import traceback\n                    print(f'  [Calibration FAILED for {name}]:')\n                    traceback.print_exc()\n\n            # Matrice de confusion\n            fig, ax = plt.subplots(figsize=(5, 4))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                        xticklabels=label_names, yticklabels=label_names, ax=ax)\n            ax.set_title(f'{name} ({target_name})  Acc={acc:.3f}  MCC={mcc:.3f}')\n            ax.set_ylabel('Vrai'); ax.set_xlabel('Pr\u00e9dit')\n            fig.tight_layout()\n            fig.savefig(output_dir / f'cm_{target_name.replace(\" \",\"_\")}_{name.replace(\" \",\"_\")}.png', dpi=150)\n            plt.show()\n\n            if loss_history:\n                lh = loss_history\n                _LOSS_METRICS = {'loss', 'logloss', 'multiclass', 'merror', 'mae', 'mse', 'rmse'}\n                is_loss = any(tok in lh['metric'].lower() for tok in _LOSS_METRICS)\n                best = int(np.argmin(lh['val']) if is_loss else np.argmax(lh['val']))\n                fig, ax = plt.subplots(figsize=(7, 4))\n                iters = range(1, len(lh['train']) + 1)\n                ax.plot(iters, lh['train'], label='Train')\n                ax.plot(iters, lh['val'],   label='Validation')\n                ax.axvline(best + 1, color='red', ls='--', alpha=0.6, label=f'Best={best+1}')\n                ax.set_title(f'Training Curve \u2014 {name} ({target_name})  ({lh[\"metric\"]})')\n                ax.set_xlabel('It\u00e9ration'); ax.set_ylabel(lh['metric'])\n                ax.legend(); fig.tight_layout()\n                fig.savefig(output_dir / f'loss_curve_{target_name.replace(\" \",\"_\")}_{name.replace(\" \",\"_\")}.png', dpi=150)\n                plt.show()\n\n            results[name] = {\n                'model': model, 'accuracy': acc, 'balanced_accuracy': bal_acc,\n                'mcc': mcc, 'kappa': kappa, 'loss_history': loss_history,\n                'confusion_matrix': cm,\n                'report': classification_report(y_test, y_pred, target_names=label_names),\n                'calibrated_model': calibrated_model,\n                'calibrated_roc_auc': calibrated_roc,\n                'optimal_threshold': opt_thr,\n                'accuracy_opt': acc_opt,\n                'balanced_accuracy_opt': bal_opt,\n                'report_opt': report_opt,\n            }\n\n        except Exception as e:\n            print(f'  ERREUR : {e}')\n            import traceback; traceback.print_exc()\n\n    return results\n\n\nprint('Helpers d\u00e9finis.')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "output_dir = Path(config.get_paths()['output_dir'])\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nhome_results = _train_binary(\n    make_models(), X_train_scaled, X_test_scaled, X_cal_scaled,\n    y_home_train, y_home_test, y_home_cal,\n    target_name='HomeWin',\n    label_names=['Not HomeWin', 'HomeWin'],\n    output_dir=output_dir\n)\nprint('\\nHomeWin \u2014 termin\u00e9.')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Courbes ROC \u2014 HomeWin (toutes les mod\u00e8les)\nfig, ax = plt.subplots(figsize=(10, 7))\nfor name, res in home_results.items():\n    m = res['model']\n    try:\n        score = (m.predict_proba(X_test_scaled)[:, 1]\n                 if hasattr(m, 'predict_proba')\n                 else m.decision_function(X_test_scaled))\n        fpr, tpr, _ = roc_curve(y_home_test, score)\n        roc_auc_val = auc(fpr, tpr)\n        res['roc_auc'] = roc_auc_val\n        ax.plot(fpr, tpr, lw=2, label=f'{name} (AUC={roc_auc_val:.3f})')\n    except Exception:\n        pass\n\nax.plot([0,1],[0,1],'k--')\nax.set_xlabel('False Positive Rate'); ax.set_ylabel('True Positive Rate')\nax.set_title('ROC \u2014 HomeWin vs Not-HomeWin')\nax.legend(loc='lower right', fontsize=8)\nfig.tight_layout()\nfig.savefig(output_dir / 'roc_HomeWin.png', dpi=150)\nplt.show()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "away_results = _train_binary(\n    make_models(), X_train_scaled, X_test_scaled, X_cal_scaled,\n    y_away_train, y_away_test, y_away_cal,\n    target_name='AwayWin',\n    label_names=['Not AwayWin', 'AwayWin'],\n    output_dir=output_dir\n)\nprint('\\nAwayWin \u2014 termin\u00e9.')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Courbes ROC \u2014 AwayWin (toutes les mod\u00e8les)\nfig, ax = plt.subplots(figsize=(10, 7))\nfor name, res in away_results.items():\n    m = res['model']\n    try:\n        score = (m.predict_proba(X_test_scaled)[:, 1]\n                 if hasattr(m, 'predict_proba')\n                 else m.decision_function(X_test_scaled))\n        fpr, tpr, _ = roc_curve(y_away_test, score)\n        roc_auc_val = auc(fpr, tpr)\n        res['roc_auc'] = roc_auc_val\n        ax.plot(fpr, tpr, lw=2, label=f'{name} (AUC={roc_auc_val:.3f})')\n    except Exception:\n        pass\n\nax.plot([0,1],[0,1],'k--')\nax.set_xlabel('False Positive Rate'); ax.set_ylabel('True Positive Rate')\nax.set_title('ROC \u2014 AwayWin vs Not-AwayWin')\nax.legend(loc='lower right', fontsize=8)\nfig.tight_layout()\nfig.savefig(output_dir / 'roc_AwayWin.png', dpi=150)\nplt.show()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def _summary_df(res_dict):\n    rows = []\n    for name, res in res_dict.items():\n        rows.append({\n            'Mod\u00e8le':    name,\n            'Accuracy':  res['accuracy'],\n            'Bal. Acc':  res['balanced_accuracy'],\n            'MCC':       res['mcc'],\n            'Kappa':     res['kappa'],\n            'ROC-AUC':   res.get('roc_auc', np.nan),\n        })\n    return pd.DataFrame(rows).sort_values('Accuracy', ascending=False)\n\nhome_df = _summary_df(home_results)\naway_df = _summary_df(away_results)\n\npd.options.display.float_format = '{:.4f}'.format\nprint('=== HomeWin vs Not-HomeWin ===')\nprint(home_df.to_string(index=False))\nprint('\\n=== AwayWin vs Not-AwayWin ===')\nprint(away_df.to_string(index=False))\n\n# Graphiques comparatifs\nfor label, df in [('Home Win', home_df), ('Away Win', away_df)]:\n    model_names = df['Mod\u00e8le'].tolist()\n    x, w = np.arange(len(model_names)), 0.28\n\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    fig.suptitle(f'Comparaison mod\u00e8les \u2014 {label}', fontsize=13)\n\n    ax = axes[0]\n    ax.bar(x - w/2, df['Accuracy'], w, label='Accuracy',     color='steelblue')\n    ax.bar(x + w/2, df['Bal. Acc'], w, label='Balanced Acc', color='coral')\n    ax.set_xticks(x); ax.set_xticklabels(model_names, rotation=30, ha='right')\n    ax.set_ylim([0, 1]); ax.set_ylabel('Score'); ax.legend()\n    ax.set_title('Accuracy vs Balanced Accuracy')\n\n    ax = axes[1]\n    ax.bar(x, df['ROC-AUC'].fillna(0), color='mediumseagreen')\n    ax.axhline(0.5, color='red', ls='--', alpha=0.5, label='Random')\n    ax.set_xticks(x); ax.set_xticklabels(model_names, rotation=30, ha='right')\n    ax.set_ylim([0, 1]); ax.set_ylabel('ROC-AUC'); ax.legend()\n    ax.set_title('ROC-AUC par mod\u00e8le')\n\n    fig.tight_layout()\n    fig.savefig(output_dir / f'summary_comparison_{label.replace(\" \",\"_\")}.png', dpi=150)\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "metrics_file = output_dir / 'metrics_results_Binary.txt'\nsorted_home = sorted(home_results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\nsorted_away = sorted(away_results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n\nwith open(metrics_file, 'w') as f:\n    for target_label, sorted_res in [('HomeWin', sorted_home), ('AwayWin', sorted_away)]:\n        f.write(f'\\n{\"=\"*70}\\n=== {target_label} ===\\n{\"=\"*70}\\n\\n')\n\n        # Ranking table\n        f.write('=== RANKING TABLE ===\\n')\n        f.write(f\"{'Mod\u00e8le':<22} {'Accuracy':>9} {'Bal.Acc':>9} {'MCC':>8} {'Kappa':>8} {'ROC-AUC':>9}\\n\")\n        f.write('-' * 70 + '\\n')\n        for name, res in sorted_res:\n            roc_str = f\"{res['roc_auc']:.4f}\" if res.get('roc_auc') is not None else '  N/A  '\n            f.write(f\"{name:<22} {res['accuracy']:>9.4f} {res['balanced_accuracy']:>9.4f} \"\n                    f\"{res['mcc']:>8.4f} {res['kappa']:>8.4f} {roc_str:>9}\\n\")\n        f.write('\\n')\n\n        # Calibrated vs Uncalibrated AUC\n        f.write('=== CALIBRATED vs UNCALIBRATED ROC-AUC ===\\n')\n        f.write(f\"{'Mod\u00e8le':<22} {'Uncalibrated':>14} {'Calibrated':>12} {'Delta':>8}\\n\")\n        f.write('-' * 60 + '\\n')\n        for name, res in sorted_res:\n            raw = res.get('roc_auc')\n            cal = res.get('calibrated_roc_auc')\n            raw_s = f\"{raw:.4f}\" if raw is not None else \"   N/A\"\n            cal_s = f\"{cal:.4f}\" if cal is not None else \"   N/A\"\n            delta_s = f\"{cal - raw:+.4f}\" if (raw is not None and cal is not None) else \"   N/A\"\n            f.write(f\"{name:<22} {raw_s:>14} {cal_s:>12} {delta_s:>8}\\n\")\n        f.write('\\n')\n\n        # Optimised threshold\n        f.write('=== OPTIMISED THRESHOLD METRICS ===\\n')\n        f.write(f\"{'Mod\u00e8le':<22} {'Acc(default)':>13} {'Acc(opt)':>10} {'BalAcc(default)':>16} {'BalAcc(opt)':>12}\\n\")\n        f.write('-' * 77 + '\\n')\n        for name, res in sorted_res:\n            acc_o_s = f\"{res['accuracy_opt']:.4f}\" if res.get('accuracy_opt') is not None else \"   N/A\"\n            bal_o_s = f\"{res['balanced_accuracy_opt']:.4f}\" if res.get('balanced_accuracy_opt') is not None else \"   N/A\"\n            f.write(f\"{name:<22} {res['accuracy']:>13.4f} {acc_o_s:>10} \"\n                    f\"{res['balanced_accuracy']:>16.4f} {bal_o_s:>12}\\n\")\n        f.write('\\n')\n\n        # Detailed\n        f.write('=== DETAILED RESULTS ===\\n\\n')\n        for name, res in sorted_res:\n            f.write(f'Mod\u00e8le : {name}\\n')\n            f.write('-' * 30 + '\\n')\n            f.write(f\"Accuracy          : {res['accuracy']:.4f}\\n\")\n            f.write(f\"Balanced Accuracy : {res['balanced_accuracy']:.4f}\\n\")\n            f.write(f\"MCC               : {res['mcc']:.4f}\\n\")\n            f.write(f\"Cohen's Kappa     : {res['kappa']:.4f}\\n\")\n            if res.get('roc_auc') is not None:\n                f.write(f\"ROC-AUC           : {res['roc_auc']:.4f}\\n\")\n            if res.get('calibrated_roc_auc') is not None:\n                f.write(f\"Calibrated ROC-AUC: {res['calibrated_roc_auc']:.4f}\\n\")\n            if res.get('accuracy_opt') is not None:\n                f.write(f\"Accuracy (opt)    : {res['accuracy_opt']:.4f}\\n\")\n                f.write(f\"Bal.Acc  (opt)    : {res['balanced_accuracy_opt']:.4f}\\n\")\n            f.write('\\nClassification Report :\\n')\n            f.write(res['report'])\n            if res.get('report_opt'):\n                f.write('\\nClassification Report (Optimised Threshold) :\\n')\n                f.write(res['report_opt'])\n            if res.get('optimal_threshold') is not None:\n                thr, f1 = res['optimal_threshold']\n                f.write(f'\\nOptimal Threshold : {thr:.4f}  (best F1={f1:.4f})\\n')\n            f.write('\\nConfusion Matrix :\\n')\n            f.write(str(res['confusion_matrix']))\n            lh = res.get('loss_history')\n            if lh:\n                _LOSS_METRICS = {'loss', 'logloss', 'multiclass', 'merror', 'mae', 'mse', 'rmse'}\n                is_loss = any(tok in lh['metric'].lower() for tok in _LOSS_METRICS)\n                best = int(np.argmin(lh['val']) if is_loss else np.argmax(lh['val']))\n                f.write(f\"\\nTraining curve: metric={lh['metric']}, best_iter={best+1}, \"\n                        f\"final_train={lh['train'][-1]:.4f}, final_val={lh['val'][-1]:.4f}\\n\")\n            f.write('\\n' + '=' * 70 + '\\n')\n\nprint(f'M\u00e9triques sauvegard\u00e9es : {metrics_file}')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}